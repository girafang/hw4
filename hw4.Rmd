---
title: "Homework 4"
author: "PSTAT 131 John Wei"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Resampling

For this assignment, we will continue working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

```{r}
library(tidymodels)
library(tidyverse)
library(ISLR) 
library(ISLR2) 
library(discrim)
library(poissonreg)
library(corrr)
library(corrplot)
library(klaR) 
library(pROC)
library(tinytex)
set.seed(4167)
```

```{r}
titanic <- read_csv("titanic.csv")
```

```{r}
titanic
titanic$survived <- factor(titanic$survived)
titanic$pclass <- factor(titanic$pclass)
titanic <- titanic %>% arrange(desc(survived))
```

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

Create a recipe for this dataset **identical** to the recipe you used in Homework 3.

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

```{r}
titanic_split <- initial_split(titanic, prop = 0.8,
                                strata = survived)
titanic_train <- training(titanic_split)
titanic_test <- testing(titanic_split)
c(dim(titanic_train),dim(titanic_test))
```

891 total observations, split 80-20.

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}
titanic_folds <- vfold_cv(titanic_train, v = 10)
titanic_folds
```
### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?

In question 2 the training set is divided into 10 groups of similar size. This lets us measure model performance without needing to predict the entire training set. If the entire training set was used we would then be using bootstrapping. 

### Question 4

Set up workflows for 3 models:

```{r}
titanic_recipe <- recipe(survived ~ pclass + sex + age + 
                           sib_sp + parch + fare, titanic_train) %>% 
  step_impute_linear(age, impute_with = imp_vars(sib_sp)) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_interact(~ starts_with("sex"):age + age:fare)
```

1. A logistic regression with the `glm` engine;

```{r}
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(titanic_recipe)
```

2. A linear discriminant analysis with the `MASS` engine;

```{r}
lda_mod <- discrim_linear() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
lda_wkflow <- workflow() %>% 
  add_model(lda_mod) %>% 
  add_recipe(titanic_recipe)
```

3. A quadratic discriminant analysis with the `MASS` engine.

```{r}
qda_mod <- discrim_quad() %>% 
  set_mode("classification") %>% 
  set_engine("MASS")
qda_wkflow <- workflow() %>% 
  add_model(qda_mod) %>% 
  add_recipe(titanic_recipe)
```

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

10 folds for each model; 30 total models will be fitted to data.

### Question 5

Fit each of the models created in Question 4 to the folded data.

```{r}
log_fit <- fit_resamples(log_wkflow, titanic_folds)
lda_fit <- fit_resamples(lda_wkflow, titanic_folds)
qda_fit <- fit_resamples(qda_wkflow, titanic_folds)
```

### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

```{r}
collect_metrics(log_fit)
```

```{r}
collect_metrics(lda_fit)
```

```{r}
collect_metrics(qda_fit)
```

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

Log model performed best as it had the highest mean accuracy and closest to lowest standard error.

### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

```{r}
log_fit1 <- fit(log_wkflow, titanic_train)
log_fit1 %>% tidy()
```

### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

```{r}
predict(log_fit1, new_data = titanic_test, type = "class") %>% 
  bind_cols(titanic_test %>% dplyr::select(survived)) %>% 
  accuracy(truth = survived, estimate = .pred_class)
```
```{r}
augment(log_fit1, new_data = titanic_test) %>%
  roc_curve(survived, .pred_Yes) %>%
  autoplot()
```
The model did well; the testing accuracy was similar to the average accuracy (higher than the other models) and the area under the curve is large.